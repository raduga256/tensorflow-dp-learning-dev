{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "code_example_schuler_baseline_various_mixes-v2.7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raduga256/tensorflow-dp-learning-dev/blob/main/v2.7/code_example_schuler_baseline_various_mixes_v2_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "monitor='val_accuracy'\n",
        "epochs=30\n",
        "batch_size=16\n",
        "input_shape=(128, 128, 3) # please resize it to (224,224,3) if you have enough RAM\n",
        "Verbose=True"
      ],
      "metadata": {
        "id": "6DH0gsiwyxgN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JXWocLnz38N"
      },
      "source": [
        "This source code requires a **HIGH RAM** machine.\n",
        "\n",
        "You might need to install this on your system:\n",
        "\n",
        "apt-get install python3-opencv git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQKpflNl7m63",
        "outputId": "f2d664b3-dba4-4f01-828d-927f41494747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('k'):\n",
        "  !git clone -b development14 https://github.com/joaopauloschuler/k-neural-api.git k\n",
        "else:\n",
        "  !cd k && git pull\n",
        "\n",
        "!cd k && pip install ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'k'...\n",
            "remote: Enumerating objects: 1751, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 1751 (delta 106), reused 98 (delta 48), pack-reused 1588\u001b[K\n",
            "Receiving objects: 100% (1751/1751), 15.62 MiB | 23.41 MiB/s, done.\n",
            "Resolving deltas: 100% (1219/1219), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/k\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from cai==0.1.5) (1.3.5)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from cai==0.1.5) (0.18.3)\n",
            "Requirement already satisfied: opencv-python>=4.1.2.30 in /usr/local/lib/python3.7/dist-packages (from cai==0.1.5) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0numpy in /usr/local/lib/python3.7/dist-packages (from cai==0.1.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.2.30->cai==0.1.5) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22.0->cai==0.1.5) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22.0->cai==0.1.5) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.22.0->cai==0.1.5) (1.15.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.5) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.5) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.5) (1.3.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.5) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.5) (2021.11.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.5) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.5) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->cai==0.1.5) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->cai==0.1.5) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->cai==0.1.5) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->cai==0.1.5) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0numpy->cai==0.1.5) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0numpy->cai==0.1.5) (1.1.0)\n",
            "Building wheels for collected packages: cai\n",
            "  Building wheel for cai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cai: filename=cai-0.1.5-py3-none-any.whl size=57415 sha256=51ec0be3e07008cf7635f6f5d13bfc3c705b6eb58cb4534c4616fed38b922985\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p4zmm6oo/wheels/c1/8a/57/56dbba25eff58e52e5365435c4fa102ad8d6f9787d3b4db13a\n",
            "Successfully built cai\n",
            "Installing collected packages: cai\n",
            "Successfully installed cai-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = 'REPLACE_WITH_YOUR_FILE_ID'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"
      ],
      "metadata": {
        "id": "y5tx42WotXaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FWmCCX96ndE"
      },
      "source": [
        "import sys\n",
        "print(\"Python version\")\n",
        "print (sys.version)\n",
        "print(\"Version info.\")\n",
        "print (sys.version_info)\n",
        "\n",
        "import skimage\n",
        "print('skimage version',  skimage.__version__)\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import sys\n",
        "import cai\n",
        "import cai.datasets\n",
        "import cai.densenet\n",
        "import cai.util\n",
        "import cai.models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WqdOtor61VZ"
      },
      "source": [
        "url_zip_file=\"https://data.mendeley.com/public-files/datasets/tywbtsjrjv/files/d5652a28-c1d8-4b76-97f3-72fb80f94efc/file_downloaded\"\n",
        "local_zip_file=\"plant_leaf.zip\"\n",
        "expected_folder_name=\"plant_leaf\"\n",
        "cai.datasets.download_zip_and_extract(\n",
        "    url_zip_file=url_zip_file, local_zip_file=local_zip_file, \n",
        "    expected_folder_name=expected_folder_name, Verbose=Verbose)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yn9mmLAxvlK"
      },
      "source": [
        "import random\n",
        "import os\n",
        "import multiprocessing\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.utils.class_weight\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2LGtzGBx5IR"
      },
      "source": [
        "!rm -r plant_leaf/Plant_leave_diseases_dataset_without_augmentation/Background_without_leaves -R\n",
        "data_dir = \"plant_leaf/Plant_leave_diseases_dataset_without_augmentation/\"\n",
        "print(os.listdir(data_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUfiL_BgqOct"
      },
      "source": [
        "def compiled_two_path_inception_v3(\n",
        "  classes=1000, \n",
        "  input_shape=input_shape,\n",
        "  two_paths_first_block=False,\n",
        "  two_paths_second_block=False,\n",
        "  max_mix_idx=10):\n",
        "  base_model = cai.models.two_path_inception_v3(\n",
        "    include_top=False,\n",
        "    weights=None,\n",
        "    input_shape=input_shape,\n",
        "    pooling=None,\n",
        "    two_paths_first_block=two_paths_first_block,\n",
        "    two_paths_second_block=two_paths_second_block,\n",
        "    max_mix_idx=max_mix_idx)\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dense(38, name='preprediction')(x)\n",
        "  predictions = Activation('softmax',name='prediction')(x)\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "  optimizer = 'sgd',\n",
        "  metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_1lHVDc082Z"
      },
      "source": [
        "train_x, val_x, test_x, train_y, val_y, test_y, classweight, classes = cai.datasets.load_images_from_folders(seed=7, root_dir=data_dir, lab=False, \n",
        "  verbose=Verbose, bipolar=False, base_model_name='plant_leaf',\n",
        "  training_size=0.6, validation_size=0.2, test_size=0.2,\n",
        "  target_size=(input_shape[0],input_shape[1]), \n",
        "  has_training=True, has_validation=True, has_testing=True, \n",
        "  smart_resize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UosoQy_c1_4M"
      },
      "source": [
        "print(train_x.shape,val_x.shape,test_x.shape)\n",
        "print(train_y.shape,val_y.shape,test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-q_V3dJ3BhT"
      },
      "source": [
        "for max_mix_idx in [5]: # range(-1,10,1):\n",
        "    basefilename = 'Schuler-baseline-v2.7-'+str(max_mix_idx)\n",
        "    print('Running: '+basefilename)\n",
        "    model = compiled_two_path_inception_v3(\n",
        "      classes=38,\n",
        "      input_shape=input_shape,\n",
        "      two_paths_first_block=False,\n",
        "      two_paths_second_block=False,\n",
        "      max_mix_idx=max_mix_idx)\n",
        "    best_result_file_name = basefilename+'-best-result.hdf5'\n",
        "    save_best = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=best_result_file_name, \n",
        "      monitor=monitor, \n",
        "      verbose=1, \n",
        "      save_best_only=True, \n",
        "      save_weights_only=False, \n",
        "      mode='max', \n",
        "      save_freq='epoch')\n",
        "    history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n",
        "      validation_data=(val_x,val_y),\n",
        "      callbacks=[save_best], \n",
        "      class_weight = classweight\n",
        "    )\n",
        "    print('Testing Last Model: '+basefilename)\n",
        "    evaluated = model.evaluate(test_x,test_y)\n",
        "    for metric, name in zip(evaluated,[\"loss\",\"acc\",\"top 5 acc\"]):\n",
        "      print(name,metric)\n",
        "    print('Best Model Results: '+basefilename)\n",
        "    model = keras.models.load_model(best_result_file_name, custom_objects={'CopyChannels': cai.layers.CopyChannels})\n",
        "    evaluated = model.evaluate(test_x,test_y)\n",
        "    for metric, name in zip(evaluated,[\"loss\",\"acc\",\"top 5 acc\"]):\n",
        "      print(name,metric)\n",
        "    print('Finished: '+basefilename)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}